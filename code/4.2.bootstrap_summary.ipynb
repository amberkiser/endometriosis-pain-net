{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile bootstrap results to get point estimates and confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv('../results/bayes_net/bootstrap_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstrap = 1000\n",
    "bootstrap_metrics = pd.DataFrame()\n",
    "for i in range(1, n_bootstrap+1):\n",
    "    final_predictions = predictions.loc[predictions['iteration'] == i]\n",
    "    report_dict = classification_report(final_predictions['y_true'].values,\n",
    "                                        final_predictions['y_pred'].values, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    n_classes = 5\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    sen = dict()\n",
    "    spec = dict()\n",
    "    for j in range(n_classes):\n",
    "        y_test_bin = np.int32(final_predictions['y_true'].values == j)\n",
    "        y_pred_bin = np.int32(final_predictions['y_pred'].values == j)\n",
    "        y_score = final_predictions['y_prob_%d' %j].values\n",
    "        fpr[j], tpr[j], _ = roc_curve(y_test_bin, y_score, pos_label=1)\n",
    "        roc_auc[j] = auc(fpr[j], tpr[j])\n",
    "        \n",
    "        tp = len(np.where((y_pred_bin == 1) & (y_test_bin == y_pred_bin))[0])\n",
    "        fp = len(np.where((y_pred_bin == 1) & (y_test_bin != y_pred_bin))[0])\n",
    "        tn = len(np.where((y_pred_bin == 0) & (y_test_bin == y_pred_bin))[0])\n",
    "        fn = len(np.where((y_pred_bin == 0) & (y_test_bin != y_pred_bin))[0])\n",
    "        sen[j] = tp / (tp+fn)\n",
    "        spec[j] = tn / (tn+fp)\n",
    "\n",
    "    temp_metrics = pd.DataFrame({'iteration': [i],\n",
    "                                 'endo_precision': [report_dict['1']['precision']],\n",
    "                                 'endo_f1': [report_dict['1']['f1-score']],\n",
    "                                 'endo_auc': [roc_auc[1]],\n",
    "                                 'endo_sen': [sen[1]],\n",
    "                                 'endo_spec': [spec[1]],\n",
    "                                 'accuracy': [report_dict['accuracy']]})\n",
    "    bootstrap_metrics = pd.concat([bootstrap_metrics, temp_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summary = pd.DataFrame()\n",
    "for col in bootstrap_metrics.drop(columns='iteration').columns.values:\n",
    "    data = bootstrap_metrics[col].values\n",
    "    \n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    \n",
    "    ci = stats.norm.interval(alpha=0.95, loc=mean, scale=se)\n",
    "\n",
    "    temp_summary = pd.DataFrame({'metric': [col],\n",
    "                                 'mean': [mean],\n",
    "                                 'se': [se],\n",
    "                                 'ci_low': [ci[0]],\n",
    "                                 'ci_high': [ci[1]]})\n",
    "    metrics_summary = pd.concat([metrics_summary, temp_summary])\n",
    "metrics_summary.to_csv('../results/bayes_net/metrics_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_probabilites = pd.read_csv('../results/bayes_net/bootstrap_probabilities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_summary = pd.DataFrame()\n",
    "\n",
    "for node in set(bootstrap_probabilites['node'].values):\n",
    "    for col in bootstrap_probabilites.drop(columns=['iteration', 'node']):\n",
    "        data = bootstrap_probabilites.loc[bootstrap_probabilites['node'] == node, col].values\n",
    "\n",
    "        mean = np.mean(data)\n",
    "        se = stats.sem(data)\n",
    "\n",
    "        ci = stats.norm.interval(alpha=0.95, loc=mean, scale=se)\n",
    "\n",
    "        temp_prob = pd.DataFrame({'node': [node],\n",
    "                                  'metric': [col],\n",
    "                                  'mean': [mean],\n",
    "                                  'se': [se],\n",
    "                                  'ci_low': [ci[0]],\n",
    "                                  'ci_high': [ci[1]]})\n",
    "        prob_summary = pd.concat([prob_summary, temp_prob])\n",
    "prob_summary.to_csv('../results/bayes_net/probabilities_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
